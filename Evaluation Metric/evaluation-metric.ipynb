{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7927028,"sourceType":"datasetVersion","datasetId":4524856},{"sourceId":7981394,"sourceType":"datasetVersion","datasetId":4697663},{"sourceId":7992115,"sourceType":"datasetVersion","datasetId":4572300}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T06:20:39.209264Z","iopub.execute_input":"2024-04-12T06:20:39.209593Z","iopub.status.idle":"2024-04-12T06:20:40.136478Z","shell.execute_reply.started":"2024-04-12T06:20:39.209566Z","shell.execute_reply":"2024-04-12T06:20:40.135658Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/marathi-summarization-dataset/marathi-marathi_val.jsonl\n/kaggle/input/marathi-summarization-dataset/marathi-marathi_train.jsonl\n/kaggle/input/marathi-summarization-dataset/marathi-marathi_test.jsonl\n/kaggle/input/summarisaton-mar-emg/mar_eng_sum.csv\n/kaggle/input/english-summarization-dataset/test_final_eng.csv\n/kaggle/input/english-summarization-dataset/val_final_eng.csv\n/kaggle/input/english-summarization-dataset/train_final_eng.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Path to the JSON Lines file\nfile_path = [\"/kaggle/input/marathi-summarization-dataset/marathi-marathi_test.jsonl\", \"/kaggle/input/marathi-summarization-dataset/marathi-marathi_train.jsonl\",\"/kaggle/input/marathi-summarization-dataset/marathi-marathi_val.jsonl\"]\n\n\n# Initialize an empty list to hold the JSON objects\ndata = []\n\n# Open the file and read line by line\nfor i in range(3):\n    data.append([])\n    with open(file_path[i], 'r', encoding='utf-8') as file:\n        for line in file:\n            # Parse each line as a JSON object and append to the list\n            data[i].append(json.loads(line))\n    \n\n# Convert the list of JSON objects into a DataFrame\ndf_mr = pd.DataFrame(data[1])\ndf_test_cs_mar = pd.DataFrame(data[0])\ndf_val_cs_mar = pd.DataFrame(data[2])\n\n# Display the first few rows of the DataFrame to check if it's loaded correctly\nprint(len(df_mr))\nprint(len(df_test_cs_mar))\nprint(len(df_val_cs_mar))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T06:20:56.344110Z","iopub.execute_input":"2024-04-12T06:20:56.344914Z","iopub.status.idle":"2024-04-12T06:20:59.569443Z","shell.execute_reply.started":"2024-04-12T06:20:56.344874Z","shell.execute_reply":"2024-04-12T06:20:59.568551Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"10558\n1188\n1254\n","output_type":"stream"}]},{"cell_type":"code","source":"df_en_test=pd.read_csv(\"/kaggle/input/english-summarization-dataset/test_final_eng.csv\")\ndf_en_test.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-12T06:20:59.570958Z","iopub.execute_input":"2024-04-12T06:20:59.571514Z","iopub.status.idle":"2024-04-12T06:20:59.920078Z","shell.execute_reply.started":"2024-04-12T06:20:59.571487Z","shell.execute_reply":"2024-04-12T06:20:59.919084Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['source_url', 'target_url', 'text', 'summary'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv(\"/kaggle/input/summarisaton-mar-emg/mar_eng_sum.csv\")\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-12T06:21:04.046820Z","iopub.execute_input":"2024-04-12T06:21:04.047703Z","iopub.status.idle":"2024-04-12T06:21:04.508531Z","shell.execute_reply.started":"2024-04-12T06:21:04.047669Z","shell.execute_reply":"2024-04-12T06:21:04.507300Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['url', 'text_mr', 'summary_en', 'summary_mr', 'text_en'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"df['summary_mr'][0], df['text_mr'][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:40:07.335086Z","iopub.execute_input":"2024-04-11T10:40:07.335908Z","iopub.status.idle":"2024-04-11T10:40:07.344641Z","shell.execute_reply.started":"2024-04-11T10:40:07.335877Z","shell.execute_reply":"2024-04-11T10:40:07.343716Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('लहान वयात म्हणजे 17व्या वर्षी त्यांना सक्तीने स्कॉटलंडहून पाकिस्तानला नेण्यात आलं. तिथे नेऊन त्यांचं चुलत भावाशी लग्न लावण्यात आलं. काय झालं नक्की?',\n 'नायला अगदी लहान वयातच माझं लग्न चुलत भावाशी ठरवण्यात आलं होतं. या निर्णयाबाबत मी नेहमी अस्वस्थ असे. स्कॉटलंडमध्ये राहणाऱ्या 30वर्षीय नायला यांनी बीबीसी स्कॉटलंडशी \\'द नाइन\\' कार्यक्रमात बोलताना याबद्दल सांगितलं. नायला सांगतात, \"भावाशी लग्न होणार आहे हे मला लहानपणापासूनच माहिती होतं. याची आठवण येताच मला कसंतरी होत असे. पाश्चिमात्य संस्कृती अंगवळणी पडून मी त्यासारखं वागेन अशी भीती माझ्या आईवडिलांना वाटत असे. पाश्चिमात्य संस्कृतीपासून मला दूर राखण्याचं काम आपण करत आहोत असं त्यांना वाटे.\" मीरपूर मुस्लीम कुटुंबात नायला यांचं बालपण व्यतीत झालं. घरचं वातावरण कर्मठ होतं. मी स्वत:चे विचार मुक्तपणे मांडू इच्छित होते. एक वेगळ्याच प्रकारचं आयुष्य मला जगायचं होतं. जेव्हा पाकिस्तानला जावं लागलं 17व्या वर्षी पाकिस्तानला जावं लागल्याची आठवण नायला आजही विसरलेल्या नाहीत. त्याबद्दल त्या विस्ताराने सांगतात, \"तू पाप केलं आहेस असं म्हणतच त्यांनी बोलायला सुरुवात केली. तुला आता भावाशीच लग्न करावं लागेल. तू कुटुंबाची प्रतिष्ठा धुळीस मिळवली आहेस. हे लग्न करून तुझी अब्रू वाचवू शकतेस\" नायला सुरुवातीला हे करायला नायला यांनी नकार दिला. मात्र घरच्यांनी सातत्याने दबाव आणला. या दडपणाला शरण जात नायला यांनी घरच्यांचं ऐकलं. \"घरचे शांत व्हावेत असं मला वाटत होतं. त्यांना हवं तसं वागल्यावर माझी ससेहोलपट झाली. मेंदू आणि मन सुन्न होत असे. मला असहाय्य वाटत असे,\" असं नायला सांगतात. घर सोडलं पाकिस्तानमध्ये पाच आठवडे राहिल्यानंतर नायला एकट्याच स्कॉटलंडला परतल्या. त्यांचे पती काही दिवसांनंतर येणार होते. मात्र काही महिन्यात नायला आपलं घर सोडून मैत्रिणीच्या घरी राहायला गेल्या. नायला त्यांनी सांगितलं, \"मी सामान घेतलं आणि पळ काढला. एक वर्षापेक्षा जास्त काळ घरापासून दूर होते. घरचे, नातेवाईक आणि समाजातल्या लोकांनी प्रचंड टीका केली.\" मी रस्त्यानं जातांना माझ्यावर विचित्र टिकाटिपण्णी व्हायला लागली. नातेवाईकांना आणि भावाबहिणीला भेटता येणार नाही, असं नायला यांना सांगण्यात आलं. अशी अवस्था झालेली की सगळं जग तुमच्यावर रुष्ट झालं आहे, असं नायला सांगतात. स्वतंत्र मुस्लीम महिला एका वर्षानंतर त्या घरी (माहेरी) परतल्या. त्यावेळी त्या मानसिकदृष्ट्या पूर्णपणे कोलमडल्या होत्या. पण आश्चर्यकारकरित्या त्यांच्या घरच्यांनी त्यांना आपलसं केलं. परंतु समाजाने त्यांना नाकारलं. हे अवघड होतं परंतु आम्ही ते साध्य केलं. धर्मापेक्षा आम्ही प्रेमाला प्राधान्य दिलं. त्यानंतर नायला यांचा घटस्फोट झाला. पुढचं शिक्षण घेण्यासाठी त्यांनी अबरदीन विद्यापीठात प्रवेश घेतला. तेव्हापासून मी एक स्वतंत्र मुस्लीम महिला आहे, असं नायला सांगतात. कोणत्याही माणसाचं जबरदस्तीने लग्न लावणं कायदेशीर गुन्हा आहे. शारीरिक स्वरुपाचं असो की मानसिक तसंच आर्थिक दडपण आणून लग्न करायला भाग पाडणं कायदेशीरदृष्ट्या गुन्हाच आहे. ब्रिटन सरकारच्या आकडेवारीनुसार गेल्या वर्षी सक्तीने लग्न लावून देणारी 1,764 प्रकरणं समोर आली आहेत. स्कॉटलंडमध्ये 2017 मध्ये जबरदस्तीने लग्न लावून देण्याच्या 18 घटना समोर आल्या होत्या. गेल्या वर्षी हे प्रमाण वाढून 30वर गेलं होतं. एखाद्या व्यक्तीचं सक्तीने लग्न लावून देणं हा कायद्याने गुन्हा आहे याची नागरिकांना जाणीव नसल्याचं ब्रिटन सरकारच्या या गुन्ह्यांची चौकशी करणाऱ्या विभागाने सांगितलं. जबरदस्तीने लग्न लावण्याच्या प्रश्नावर कोणताही ठोस उपाय आपण काढू शकलेलो नाही. शिक्षण आणि जागरुकता या दोन गोष्टींची यात निर्णायक भूमिका आहे. मुलीचं जबरदस्तीने कोणाशी तरी लग्न लावून आपण तिला किती दु:ख देत आहोत याची घरच्यांना जाणीव व्हायला हवी. मुलीचं जबरदस्तीने लग्न लावून तिला मानसिक आणि शारीरिकदृष्ट्या किती कमकुवत करत आहोत याची त्यांना कल्पनाच नाही. हे वाचलंत का? (बीबीसी मराठीचे सर्व अपडेट्स मिळवण्यासाठी तुम्ही आम्हाला फेसबुक, इन्स्टाग्राम, यूट्यूब, ट्विटर वर फॉलो करू शकता.\\'बीबीसी विश्व\\' रोज संध्याकाळी 7 वाजता JioTV अॅप आणि यूट्यूबवर नक्की पाहा.)')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndef generate_relevance_score(model_path, text, summary):\n    \"\"\"\n    Generate a relevance score for a given text and summary pair using a fine-tuned MuRIL model.\n\n    Args:\n        model_path (str): Path to the fine-tuned MuRIL model.\n        text (str): The input text.\n        summary (str): The corresponding summary.\n\n    Returns:\n        float: The relevance score.\n    \"\"\"\n    # Load the tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Tokenize the text and summary\n    inputs = tokenizer(text, summary, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n\n    # Generate predictions\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probabilities = torch.softmax(logits, dim=1)\n\n    # Assuming the second column of the output represents the \"relevance\" class\n    relevance_score = probabilities[:, 1].item()  # Convert to Python float\n\n    return relevance_score\n\n# Example usage\nmodel_path = \"october-sd/MuRIL_relevance\"\ntext = df['text_mr'][1]\nsummary = df['summary_mr'][1]\n\nrelevance_score = generate_relevance_score(model_path, text, summary)\nprint(f\"Relevance Score: {relevance_score:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:40:14.761860Z","iopub.execute_input":"2024-04-11T10:40:14.762571Z","iopub.status.idle":"2024-04-11T10:40:37.853604Z","shell.execute_reply.started":"2024-04-11T10:40:14.762538Z","shell.execute_reply":"2024-04-11T10:40:37.852635Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09a9a7fcfdd4a3e9ec811f8cab37139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349ce97e67fb4fc387a2a156a8c09f85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"915c5c2d5af24191ba42c22b30cc0d7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1510d1ee7d04d86b0294b55b3225349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b10450af7fc4dac91f5cdfd4f7cd516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/950M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d91a28cfe0344a5aebce5353b669cfe"}},"metadata":{}},{"name":"stdout","text":"Relevance Score: 0.98\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndef generate_average_relevance_score(model_path, texts, summaries):\n    \"\"\"\n    Generate average relevance score for lists of texts and summaries using a fine-tuned MuRIL model.\n\n    Args:\n        model_path (str): Path to the fine-tuned MuRIL model.\n        texts (list of str): List of input texts.\n        summaries (list of str): List of corresponding summaries.\n\n    Returns:\n        float: The average relevance score.\n    \"\"\"\n    # Load the tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Initialize the list to store relevance scores\n    relevance_scores = []\n\n    # Process each text-summary pair\n    for text, summary in zip(texts, summaries):\n        # Tokenize the text and summary\n        inputs = tokenizer(text, summary, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n\n        # Generate predictions\n        with torch.no_grad():\n            outputs = model(**inputs)\n            logits = outputs.logits\n            probabilities = torch.softmax(logits, dim=1)\n\n        # Assuming the second column of the output represents the \"relevance\" class\n        relevance_score = probabilities[:, 1].item()  # Convert to Python float\n        relevance_scores.append(relevance_score)\n\n    # Calculate the average relevance score\n    average_relevance_score = sum(relevance_scores) / len(relevance_scores)\n    return average_relevance_score\n\n# Example usage\nmodel_path = \"october-sd/MuRIL_relevance\"\ntexts = list(df['text_mr'][0:100])\nsummaries = list(df['summary_mr'][0:100])\n\naverage_relevance_score = generate_average_relevance_score(model_path, texts, summaries)\nprint(f\"Average Relevance Score: {average_relevance_score:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T12:03:00.795134Z","iopub.execute_input":"2024-04-08T12:03:00.796863Z","iopub.status.idle":"2024-04-08T12:04:41.592372Z","shell.execute_reply.started":"2024-04-08T12:03:00.796800Z","shell.execute_reply":"2024-04-08T12:04:41.590761Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Average Relevance Score: 0.98\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the relevance classification pipeline\nrelevance_classifier = pipeline(\"text-classification\", model=\"october-sd/MuRIL_relevance\", tokenizer=\"google/muril-base-cased\")\n\ndef generate_relevance_score(text, summary):\n    \"\"\"\n    Generate a relevance score for a given text and summary pair using a fine-tuned MuRIL model pipeline.\n\n    Args:\n        text (str): The input text.\n        summary (str): The corresponding summary.\n\n    Returns:\n        float: The relevance score.\n    \"\"\"\n    # Combine text and summary as the input to the pipeline\n    input_pair = text + \" \" + summary  # Adjust this concatenation based on how your model was fine-tuned\n\n    # Generate predictions using the pipeline\n    results = relevance_classifier(input_pair)\n\n    # Extract the relevance score\n    # The label and score are model-specific, so adjust based on your model's output\n    relevance_score = results[0]['score'] if results[0]['label'] == 'LABEL_1' else 1 - results[0]['score']\n\n    return relevance_score\n\n# Example usage\ntext = df['text_mr'][1]\nsummary = df['summary_mr'][1]\n\nrelevance_score = generate_relevance_score(text, summary)\nprint(f\"Relevance Score: {relevance_score:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:00:34.826356Z","iopub.execute_input":"2024-04-11T11:00:34.826748Z","iopub.status.idle":"2024-04-11T11:01:57.802016Z","shell.execute_reply.started":"2024-04-11T11:00:34.826718Z","shell.execute_reply":"2024-04-11T11:01:57.800978Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-04-11 11:00:42.277622: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 11:00:42.277721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 11:00:42.412978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6864a83518bd4c358f4aea4dd01136fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/950M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854238ccbb0645c8b588aab11c1030e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5cbf6a7eae4c33902d4c754bb3cc2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add717752c0d47a59af95a57e9b09186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0929026d77394b51afe1405b4ad538f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706fa7b8188649d2a16e675d145aaece"}},"metadata":{}},{"name":"stdout","text":"Relevance Score: 0.98\n","output_type":"stream"}]},{"cell_type":"code","source":"text = df_val_cs_mar['text'][1]\nsummary = df_val_cs_mar['summary'][1]\n\nrelevance_score = generate_relevance_score(text, summary)\nprint(f\"Relevance Score: {relevance_score:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def relevance(texts, summaries):\n    model_path = \"october-sd/MuRIL_relevance\"\n    relevance_score = generate_average_relevance_score(model_path,texts, summaries)\n    return relevance_score","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:59:42.712887Z","iopub.execute_input":"2024-04-08T11:59:42.713331Z","iopub.status.idle":"2024-04-08T11:59:42.721178Z","shell.execute_reply.started":"2024-04-08T11:59:42.713298Z","shell.execute_reply":"2024-04-08T11:59:42.719444Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(relevance(texts, summaries))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:59:51.066552Z","iopub.execute_input":"2024-04-08T11:59:51.067379Z","iopub.status.idle":"2024-04-08T12:00:02.700134Z","shell.execute_reply.started":"2024-04-08T11:59:51.067330Z","shell.execute_reply":"2024-04-08T12:00:02.698648Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"0.9845685958862305\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\n\n# Load the pipeline for text classification using the fine-tuned MuRIL model\nrelevance_classifier = pipeline(\"text-classification\", model=\"october-sd/MuRIL_relevance\", tokenizer=\"google/muril-base-cased\", device=0)  # device=0 to use GPU\n\ndef generate_average_relevance_score(texts, summaries):\n    \"\"\"\n    Generate average relevance score for a list of text-summary pairs.\n\n    Args:\n        texts (list of str): List of input texts.\n        summaries (list of str): List of corresponding summaries.\n\n    Returns:\n        float: The average relevance score.\n    \"\"\"\n    scores = []\n    max_length = 512  # Maximum sequence length\n    batch_size = 8  # Adjust based on your GPU memory\n\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        batch_summaries = summaries[i:i+batch_size]\n\n        # Prepare the batch for processing\n        batch_input = []\n        for text, summary in zip(batch_texts, batch_summaries):\n            # Truncate texts and summaries to half of max_length to accommodate both\n            truncated_text = text[:max_length // 2]\n            truncated_summary = summary[:max_length // 2]\n            batch_input.append(truncated_text + \" \" + truncated_summary)  # Concatenate text and summary\n\n        # Get relevance scores for the batch\n        batch_scores = relevance_classifier(batch_input)\n        \n        # Extract and store scores\n        for score in batch_scores:\n            relevance_score = score['score'] if score['label'] == 'LABEL_1' else 1 - score['score']\n            scores.append(relevance_score)\n\n    # Calculate the average relevance score\n    average_score = sum(scores) / len(scores)\n    return average_score\n\n# Example usage\ntexts = list(df['text_mr'][0:10])\nsummaries = list(df['summary_mr'][0:10])\n\naverage_relevance_score = generate_average_relevance_score(texts, summaries)\nprint(f\"Average Relevance Score: {average_relevance_score:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:03:49.411272Z","iopub.execute_input":"2024-04-11T11:03:49.412152Z","iopub.status.idle":"2024-04-11T11:03:51.858000Z","shell.execute_reply.started":"2024-04-11T11:03:49.412119Z","shell.execute_reply":"2024-04-11T11:03:51.856989Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Average Relevance Score: 0.98\n","output_type":"stream"}]},{"cell_type":"code","source":"texts = list(df_test_cs_mar['text'][0:1000])\nsummaries = list(df_test_cs_mar['summary'][1:1001])\n\naverage_relevance_score = generate_average_relevance_score(texts, summaries)\nprint(f\"Average Relevance Score: {average_relevance_score:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:04:45.597540Z","iopub.execute_input":"2024-04-11T11:04:45.598415Z","iopub.status.idle":"2024-04-11T11:04:56.440339Z","shell.execute_reply.started":"2024-04-11T11:04:45.598383Z","shell.execute_reply":"2024-04-11T11:04:56.439211Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Average Relevance Score: 0.06\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langdetect","metadata":{"execution":{"iopub.status.busy":"2024-04-12T06:21:14.877668Z","iopub.execute_input":"2024-04-12T06:21:14.878127Z","iopub.status.idle":"2024-04-12T06:21:30.597709Z","shell.execute_reply.started":"2024-04-12T06:21:14.878099Z","shell.execute_reply":"2024-04-12T06:21:30.596711Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=3ac75085c5e8b0ff8a6834f40048f9fc91d54780a2bc6374542e64360004c838\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer, AutoModel\nfrom scipy.spatial.distance import cosine\nfrom langdetect import detect\n\n# Load NER models\ntokenizer_en_ner = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\nmodel_en_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n\ntokenizer_mr_ner = AutoTokenizer.from_pretrained(\"l3cube-pune/marathi-ner\")\nmodel_mr_ner = AutoModelForTokenClassification.from_pretrained(\"l3cube-pune/marathi-ner\")\n\n# Load LaBSE for embeddings\ntokenizer_labse = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\nmodel_labse = AutoModel.from_pretrained(\"setu4993/LaBSE\")\n\ndef detect_language(text):\n    try:\n        return detect(text)\n    except:\n        return None\n\ndef extract_named_entities(text, tokenizer_ner, model_ner, device='cpu'):\n    model_ner.to(device)\n    tokens = tokenizer_ner(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model_ner(input_ids, attention_mask=attention_mask)\n    predictions = torch.argmax(outputs.logits, dim=-1)\n    \n    named_entities = []\n    for token, pred in zip(tokens.tokens(), predictions[0].tolist()):\n        if pred != 0:  # Non-entity token\n            named_entities.append(token)\n\n    return named_entities\n\ndef generate_entity_embeddings(entities, tokenizer_emb, model_emb, device='cpu'):\n    model_emb.to(device)\n    entity_embeddings = []\n    for entity in entities:\n        encoded_entity = tokenizer_emb(entity, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n        with torch.no_grad():\n            entity_output = model_emb(**encoded_entity)\n        entity_embeddings.append(entity_output.last_hidden_state[:, 0, :].squeeze().cpu().numpy())\n    return entity_embeddings\n\ndef calculate_concept_coverage(summary_entities, text_entities, cross_lingual=False, threshold=0.7):\n    coverage_count = 0\n    for summary_entity in summary_entities:\n        for text_entity in text_entities:\n            if cross_lingual:\n                similarity = 1 - cosine(summary_entity, text_entity)\n                if similarity > threshold:\n                    coverage_count += 1\n                    break\n            else:\n                if summary_entity == text_entity:\n                    coverage_count += 1\n                    break\n    coverage = coverage_count / len(summary_entities) if summary_entities else 0\n    return coverage\n\ndef concept_coverage_pipeline(summary, text, device='cpu'):\n    lang_summary = detect_language(summary)\n    lang_text = detect_language(text)\n\n    if lang_summary == 'en' and lang_text == 'en':\n        entities_summary = extract_named_entities(summary, tokenizer_en_ner, model_en_ner, device)\n        entities_text = extract_named_entities(text, tokenizer_en_ner, model_en_ner, device)\n        coverage = calculate_concept_coverage(entities_summary, entities_text, cross_lingual=False)\n    elif lang_summary == 'mr' and lang_text == 'mr':\n        entities_summary = extract_named_entities(summary, tokenizer_mr_ner, model_mr_ner, device)\n        entities_text = extract_named_entities(text, tokenizer_mr_ner, model_mr_ner, device)\n        coverage = calculate_concept_coverage(entities_summary, entities_text, cross_lingual=False)\n    else:\n        # Cross-lingual case\n        if lang_summary == 'en':\n            entities_summary = extract_named_entities(summary, tokenizer_en_ner, model_en_ner, device)\n            entities_text = extract_named_entities(text, tokenizer_mr_ner, model_mr_ner, device)\n        else:\n            entities_summary = extract_named_entities(summary, tokenizer_mr_ner, model_mr_ner, device)\n            entities_text = extract_named_entities(text, tokenizer_en_ner, model_en_ner, device)\n\n        # Generate embeddings for cross-lingual comparison\n        summary_embeddings = generate_entity_embeddings(entities_summary, tokenizer_labse, model_labse, device)\n        text_embeddings = generate_entity_embeddings(entities_text, tokenizer_labse, model_labse, device)\n        coverage = calculate_concept_coverage(summary_embeddings, text_embeddings, cross_lingual=True, threshold=0.7)\n\n    return coverage","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.347608Z","iopub.execute_input":"2024-04-08T17:25:40.347946Z","iopub.status.idle":"2024-04-08T17:25:43.168212Z","shell.execute_reply.started":"2024-04-08T17:25:40.347919Z","shell.execute_reply":"2024-04-08T17:25:43.167436Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer, AutoModel\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.spatial.distance import cosine\nfrom langdetect import detect\n\n# Load models and tokenizers\ntokenizer_en_ner = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\nmodel_en_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\ntokenizer_mr_ner = AutoTokenizer.from_pretrained(\"l3cube-pune/marathi-ner\")\nmodel_mr_ner = AutoModelForTokenClassification.from_pretrained(\"l3cube-pune/marathi-ner\")\ntokenizer_labse = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\nmodel_labse = AutoModel.from_pretrained(\"setu4993/LaBSE\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_en_ner.to(device)\nmodel_mr_ner.to(device)\nmodel_labse.to(device)\n\ndef detect_languages(texts):\n    languages = []\n    for text in texts:\n        try:\n            lang = detect(text)\n        except:\n            lang = None\n        languages.append(lang)\n    return languages\n\ndef extract_and_embed_entities(texts, tokenizer_ner, model_ner, tokenizer_emb, model_emb):\n    inputs = tokenizer_ner(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n    model_ner.eval()\n    with torch.no_grad():\n        outputs = model_ner(**inputs)\n    predictions = torch.argmax(outputs.logits, dim=-1)\n\n    entities_batch = []\n    for i, prediction in enumerate(predictions):\n        tokens = tokenizer_ner.convert_ids_to_tokens(inputs['input_ids'][i])\n        entities = [token for token, pred in zip(tokens, prediction.tolist()) if pred != 0]\n        entities_batch.append(entities)\n\n    entity_embeddings_batch = []\n    for entities in entities_batch:\n        if entities:\n            encoded_entities = tokenizer_emb(entities, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n            model_emb.eval()\n            with torch.no_grad():\n                outputs_emb = model_emb(**encoded_entities)\n            embeddings = outputs_emb.last_hidden_state[:, 0, :].cpu().numpy()\n        else:\n            embeddings = np.array([])\n        entity_embeddings_batch.append(embeddings)\n    return entity_embeddings_batch\n\ndef calculate_coverage(entity_embeddings_summary, entity_embeddings_text, threshold=0.7):\n    coverage_scores = []\n    for summary_embeddings, text_embeddings in zip(entity_embeddings_summary, entity_embeddings_text):\n        coverage = 0\n        for summary_emb in summary_embeddings:\n            for text_emb in text_embeddings:\n                if 1 - cosine(summary_emb, text_emb) > threshold:\n                    coverage += 1\n                    break\n        coverage_scores.append(coverage / len(summary_embeddings) if len(summary_embeddings) > 0 else 0)\n    return np.mean(coverage_scores)\n\ndef concept_coverage_pipeline(texts, summaries):\n    languages_texts = detect_languages(texts)\n    languages_summaries = detect_languages(summaries)\n\n    batches = defaultdict(lambda: {'texts': [], 'summaries': []})\n    for text, summary, lang_text, lang_summary in zip(texts, summaries, languages_texts, languages_summaries):\n        batches[(lang_text, lang_summary)]['texts'].append(text)\n        batches[(lang_text, lang_summary)]['summaries'].append(summary)\n\n    overall_coverage = []\n    for (lang_text, lang_summary), batch in batches.items():\n        if lang_text == 'en' and lang_summary == 'en':\n            entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse)\n            entity_embeddings_summary = extract_and_embed_entities(batch['summaries'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse)\n        elif lang_text == 'mr' and lang_summary == 'mr':\n            entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse)\n            entity_embeddings_summary = extract_and_embed_entities(batch['summaries'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse)\n        else:\n            if lang_summary == 'en':\n                entity_embeddings_summary = e\n            # Cross-lingual casextract_and_embed_entities(batch['summaries'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse)\n                entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse)\n            else:\n                entity_embeddings_summary = extract_and_embed_entities(batch['summaries'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse)\n                entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse)\n\n        coverage = calculate_coverage(entity_embeddings_summary, entity_embeddings_text)\n        overall_coverage.append(coverage)\n\n    return np.mean(overall_coverage)\n\n# Example usage with a list of texts and summaries\ntexts = list(df['text_en'][0:10])\nsummaries = list(df['summary_en'][0:10])\naverage_coverage = concept_coverage_pipeline(texts, summaries)\nprint(f\"Average Concept Coverage: {average_coverage}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:40:25.103847Z","iopub.execute_input":"2024-04-08T17:40:25.104196Z","iopub.status.idle":"2024-04-08T17:40:30.331696Z","shell.execute_reply.started":"2024-04-08T17:40:25.104170Z","shell.execute_reply":"2024-04-08T17:40:30.330706Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Average Concept Coverage: 0.9458333333333334\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer, AutoModel\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.spatial.distance import cosine\nfrom langdetect import detect\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load models and tokenizers\ntokenizer_en_ner = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\nmodel_en_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\").to(device)\ntokenizer_mr_ner = AutoTokenizer.from_pretrained(\"l3cube-pune/marathi-ner\")\nmodel_mr_ner = AutoModelForTokenClassification.from_pretrained(\"l3cube-pune/marathi-ner\").to(device)\ntokenizer_labse = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\nmodel_labse = AutoModel.from_pretrained(\"setu4993/LaBSE\").to(device)\n\n\nBATCH_SIZE = 16\n\ndef detect_languages(texts):\n    languages = []\n    for text in texts:\n        try:\n            lang = detect(text)\n        except:\n            lang = None\n        languages.append(lang)\n    return languages\n\ndef extract_and_embed_entities(texts, tokenizer_ner, model_ner, tokenizer_emb, model_emb, device):\n    # Initialize lists for batch processing\n    entity_embeddings_batch = []\n\n    # Process texts in batches\n    for i in range(0, len(texts), BATCH_SIZE):\n        batch_texts = texts[i:i + BATCH_SIZE]\n\n        # Tokenization and model inference within torch.no_grad() to save memory\n        with torch.no_grad():\n            inputs = tokenizer_ner(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n            outputs = model_ner(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n\n        # Entity extraction and embedding generation also within torch.no_grad()\n        with torch.no_grad():\n            for i, prediction in enumerate(predictions):\n                tokens = tokenizer_ner.convert_ids_to_tokens(inputs['input_ids'][i])\n                entities = [token for token, pred in zip(tokens, prediction.tolist()) if pred != 0]\n\n                if entities:\n                    encoded_entities = tokenizer_emb(entities, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n                    outputs_emb = model_emb(**encoded_entities)\n                    embeddings = outputs_emb.last_hidden_state[:, 0, :].cpu().numpy()\n                else:\n                    embeddings = np.array([])\n                entity_embeddings_batch.append(embeddings)\n\n    return entity_embeddings_batch\n\ndef calculate_coverage(entity_embeddings_summary, entity_embeddings_text, threshold):\n    coverage_scores = []\n    for summary_embeddings, text_embeddings in zip(entity_embeddings_summary, entity_embeddings_text):\n        coverage = 0\n        for summary_emb in summary_embeddings:\n            for text_emb in text_embeddings:\n                if 1 - cosine(summary_emb, text_emb) > threshold:\n                    coverage += 1\n                    break\n        coverage_scores.append(coverage / len(summary_embeddings) if len(summary_embeddings) > 0 else 0)\n    return np.mean(coverage_scores)\n\ndef concept_coverage_pipeline(texts, summaries):\n    languages_texts = detect_languages(texts)\n    languages_summaries = detect_languages(summaries)\n\n    batches = defaultdict(lambda: {'texts': [], 'summaries': []})\n    for text, summary, lang_text, lang_summary in zip(texts, summaries, languages_texts, languages_summaries):\n        batches[(lang_text, lang_summary)]['texts'].append(text)\n        batches[(lang_text, lang_summary)]['summaries'].append(summary)\n\n    overall_coverage = []\n    for (lang_text, lang_summary), batch in batches.items():\n        if lang_text == 'en' and lang_summary == 'en':\n            threshold = 0.8\n            entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse, device)\n            entity_embeddings_summary = extract_and_embed_entities(batch['summaries'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse, device)\n        elif lang_text == 'mr' and lang_summary == 'mr':\n            threshold = 0.75\n            entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse, device)\n            entity_embeddings_summary = extract_and_embed_entities(batch['summaries'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse, device)\n        else:\n            # Cross-lingual case\n            if lang_summary == 'en':\n                threshold = 0.8\n                entity_embeddings_summary = extract_and_embed_entities(batch['summaries'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse, device)\n                entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse, device)\n            else:\n                threshold = 0.83\n                entity_embeddings_summary = extract_and_embed_entities(batch['summaries'], tokenizer_mr_ner, model_mr_ner, tokenizer_labse, model_labse, device)\n                entity_embeddings_text = extract_and_embed_entities(batch['texts'], tokenizer_en_ner, model_en_ner, tokenizer_labse, model_labse, device)\n\n        coverage = calculate_coverage(entity_embeddings_summary, entity_embeddings_text, threshold)\n        overall_coverage.append(coverage)\n        \n        c = np.mean(overall_coverage)\n#         if (lang_text == 'mr' and lang_summary == 'mr') or lang_summary == 'mr':\n#             if c > 0.5:\n#                 if c < 0.7:\n#                     c += 0.2\n#             elif c < 0.5:\n#                 if c > 0.3:\n#                     c -= 0.2\n                    \n    torch.cuda.empty_cache()\n    return c \n   \n\n# Example usage with a list of texts and summaries\n# texts = list(df['text_en'][0:10])\n# summaries = list(df['summary_en'][0:10])\n# average_coverage = concept_coverage_pipeline(texts, summaries)\n# print(f\"Average Concept Coverage: {average_coverage}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T07:24:28.129991Z","iopub.execute_input":"2024-04-12T07:24:28.130366Z","iopub.status.idle":"2024-04-12T07:24:32.433352Z","shell.execute_reply.started":"2024-04-12T07:24:28.130339Z","shell.execute_reply":"2024-04-12T07:24:32.432567Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"texts = list(df['text_en'][0:100])\nsummaries = list(df['summary_en'][1:101])\naverage_coverage = concept_coverage_pipeline(texts, summaries)\nprint(f\"Average Concept Coverage: {average_coverage}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T07:26:23.435999Z","iopub.execute_input":"2024-04-12T07:26:23.436366Z","iopub.status.idle":"2024-04-12T07:26:36.858009Z","shell.execute_reply.started":"2024-04-12T07:26:23.436340Z","shell.execute_reply":"2024-04-12T07:26:36.857018Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Average Concept Coverage: 0.21739722434913916\n","output_type":"stream"}]},{"cell_type":"code","source":"df['summary_mr'][2], df['text_mr'][4]","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:19:23.046596Z","iopub.execute_input":"2024-04-08T03:19:23.047127Z","iopub.status.idle":"2024-04-08T03:19:23.056827Z","shell.execute_reply.started":"2024-04-08T03:19:23.047080Z","shell.execute_reply":"2024-04-08T03:19:23.055400Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('जगभरात कोरोनाचा उद्रेक झालेला असताना जगाच्या पाठीवर एक असाही देश आहे ज्याने आपल्याकडे एकालाही कोरोना विषाणूची लागण झालेली नाही, असा दावा केला आहे. हा देश आहे उत्तर कोरिया.',\n 'नवीन आकडेवारीनुसार 35.70 कोटींपेक्षा जास्त मुलं युद्धजन्य परिस्थितींमध्ये राहत आहेत. 1990 साली ही संख्या 20 कोटी होती. त्यात आतापर्यंत 75 टक्क्यांनी वाढ झाली आहे. सीरिया, अफगाणिस्तान आणि सोमालिया हे देश मुलांसाठी सर्वाधिक घातक असल्याचं समजतं. मध्य-पूर्व आशियामधील बहुतांश मुलं युद्धजन्य क्षेत्रांमध्ये राहत आहेत. या भागांमध्ये दर पाच पैकी दोन मुलं जीवघेण्या हल्ल्यांपासून केवळ 50 किमी अंतरावर राहतात. आफ्रिका खंड या क्षेत्रांच्या यादीत दुसऱ्या स्थानावर आहे. युद्धजन्य क्षेत्रात राहणाऱ्या मुलांपैकी जवळजवळ निम्मी मुलं, म्हणजे साधारणतः 16.5 कोटी मुलं अतितीव्र संघर्ष क्षेत्रात असल्याचं या अहवालात सांगण्यात आलं आहे. UN ने मुलांवर होणाऱ्या अत्याचारांबाबत हे सहा मुख्य निकष लावले आहेत - \\'सेव्ह द चिल्ड्रन\\'ने UN आणि इतर संशोधन अहवालांचा आधार घेत त्यांचा अहवाल तयार केला आहे. पण कोणत्याही संघर्षात अडकलेल्या दोन्ही सैन्यांनी गोळा केलेल्या आकडेवारीत प्रचंड तफावत असल्याची टीकाही त्यांनी केली आहे. UNची अधिकृत आकडेवारी सांगते की 2010 पासून मुलांची हत्या आणि अपंगत्वासारख्या घटनांमध्ये 300 टक्क्यांनी वाढ झाली आहे. या वाढीमागचं एक महत्त्वाचं कारण म्हणजे गेल्या काही वर्षांमध्ये अशा संघर्षांनी युद्धभूमींपर्यंत मर्यादित न राहता शहरी भागामध्ये आपले पाय रोवले आहेत. त्याचबरोबर ते दीर्घ काळ चालत आहेत आणि दिवसेंदिवस किचकटही होत चालले आहेत. त्यालाच जोड दिली जाते ती जहालमतवादी गटांकडून. ते जाणीवपूर्वक मानवतावादी मदत लोकांपर्यंत पोहोचू देत नाही. यामुळेच येमेन आणि सीरिया सारख्या देशांमध्ये बराचसा भाग दीर्घकाळासाठी ओलीस ठेवला गेला आहे. \"युद्धामध्ये ओलीस ठेवणं आणि नागरिकांवर उपाशी सोडण्यासारखे डावपेच रचून त्यांना शरण आणण्याचा प्रयत्न केले जात आहे,\" असं या अहवालात सांगितलं आहे. हॉस्पिटल आणि शाळांवर होणारे हल्ले आता \"सामान्य बाब\" झाली आहे, अशी खंत हा अहवाल व्यक्त करतो. पाहा व्हीडिओ : येमेनमधला संघर्ष निर्माण करतंय मानवी संकट मुलांच्या संरक्षणासाठी आंतरराष्ट्रीय कायदे अधिक कडक केले असले तरी जगभरामध्ये मुलांवर क्रूर अत्याचार केले जात आहेत. यामध्ये मुलांची सैन्यामध्ये भरती आणि मुलांवरील लैंगिक अत्याचारासारख्या गंभीर गुन्ह्यांचा समावेश आहे. बहुतांश लैंगिक अत्याचारांची नोंद करण्यात दिरंगाई करण्यात येत आहे. मुलांची हत्या आणि अपंगत्व आणणारी रासायनिक शस्त्रं, सुरुंग स्फोट, क्लस्टर बॉम्ब यांचा वापर कमी झाला असला तर इतर धोके कायम आहेत. मुलांचा आत्मघातकी बॉम्बहल्लेखोर म्हणून वापर करून बेसुमार मनुष्यहानी करण्यात येत आहे. या व्यतिरिक्त मुलांना मूलभूत सुविधा आणि शिक्षणापासून वंचित राहावं लागत आहे आणि त्यांची उपासमार होत आहे, असं निदर्शनास आलं आहे. \"लैंगिक अत्याचारापासून आत्मघातकी हल्लांसारख्या घटना मुलांच्या नशिबी येत आहेत. त्यांची घरं, खेळाची मैदानं सध्या युद्धभूमी झाल्या आहेत,\" असं \\'सेव्ह द चिल्ड्रन\\'च्या CEO हेले थोर्निंग स्कीम्ट यांचं म्हणणं आहे. \"मुलांवर होणारे हे गुन्हे माणुसकीला काळिमा फासणारे आहेत. हे सरळसरळ आंतरराष्ट्रीय कायद्यांचं उल्लंघन आहे,\" असं त्या पुढं म्हणाल्या. त्यांनी जागतिक नेत्यांना याबाबत पावलं उचलण्याचं आवाहन केलं आहे. \\'War on Children\\' (मुलांविरुद्ध युद्ध) नावाचा हा अहवाल \\'सेव्ह द चिल्ड्रन\\' आणि \\'Peace Research Institute Oslo\\' (PRIO), या दोन संस्थांनी मिळून तयार केला आहे. यात त्यांनी 1995-2016 दरम्यान जगामध्ये झालेल्या हिंसाचाराची नोंद घेतली आहे. म्यानमारमधून पळालेल्या मोहसिनाने बांगलादेशमध्ये पाहोचल्यावर अन्वरला जन्म दिला. 2017ची आकडेवारी अपूर्ण असल्यानं म्यानमारमध्ये नुकत्याच झालेल्या हिंसाचाराची आकडेवारी समोर येऊ शकली नाही. येत्या शु्क्रवारी जर्मनीमध्ये म्युनिक सिक्युरिटी कॉन्फरन्स होत आहे. या महत्त्वाच्या परिषदेच्या अगोदर हा अहवाल प्रसिद्ध करण्यात आला आहे. मुलांच्या संरक्षणासाठी या परिषदेला योग्य पाऊलं उचलण्याची संधी आहे, असं या संस्थेचं म्हणणं आहे. हे वाचलं का? (बीबीसी मराठीचे सर्व अपडेट्स मिळवण्यासाठी तुम्ही आम्हाला फेसबुक, इन्स्टाग्राम, यूट्यूब, ट्विटर वर फॉलो करू शकता.)')"},"metadata":{}}]},{"cell_type":"code","source":"coverage_en_en = concept_coverage_pipeline(df['summary_en'][2], df['text_en'][2])","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:27:08.176128Z","iopub.execute_input":"2024-04-08T17:27:08.176525Z","iopub.status.idle":"2024-04-08T17:27:10.225232Z","shell.execute_reply.started":"2024-04-08T17:27:08.176491Z","shell.execute_reply":"2024-04-08T17:27:10.224355Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(coverage_en_en)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:27:12.133069Z","iopub.execute_input":"2024-04-08T17:27:12.133413Z","iopub.status.idle":"2024-04-08T17:27:12.138071Z","shell.execute_reply.started":"2024-04-08T17:27:12.133387Z","shell.execute_reply":"2024-04-08T17:27:12.137142Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\n# Load the LaBSE model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\nmodel = AutoModel.from_pretrained(\"setu4993/LaBSE\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:32:54.783691Z","iopub.execute_input":"2024-04-08T03:32:54.784146Z","iopub.status.idle":"2024-04-08T03:32:58.761755Z","shell.execute_reply.started":"2024-04-08T03:32:54.784104Z","shell.execute_reply":"2024-04-08T03:32:58.760469Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from scipy.spatial.distance import cosine\n\ndef semantic_similarity(text1, text2, tokenizer, model):\n    # Tokenize and encode the texts\n    inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n\n    # Generate embeddings\n    with torch.no_grad():\n        embeddings1 = model(**inputs1).pooler_output\n        embeddings2 = model(**inputs2).pooler_output\n\n    # Normalize the embeddings\n    embeddings1 = torch.nn.functional.normalize(embeddings1)\n    embeddings2 = torch.nn.functional.normalize(embeddings2)\n\n    # Compute cosine similarity\n    sim = torch.mm(embeddings1, embeddings2.transpose(0, 1))\n    \n    return sim.item()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:34:11.390756Z","iopub.execute_input":"2024-04-08T03:34:11.391467Z","iopub.status.idle":"2024-04-08T03:34:11.400439Z","shell.execute_reply.started":"2024-04-08T03:34:11.391428Z","shell.execute_reply":"2024-04-08T03:34:11.398967Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"original_text = df['text_en'][2]\ngenerated_summary = df['summary_en'][3]\n# Calculate the semantic similarity\nsimilarity_score = semantic_similarity(original_text, generated_summary, tokenizer, model)\nprint(f\"Semantic Similarity Score: {similarity_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:39:34.587294Z","iopub.execute_input":"2024-04-08T03:39:34.587741Z","iopub.status.idle":"2024-04-08T03:39:35.613385Z","shell.execute_reply.started":"2024-04-08T03:39:34.587704Z","shell.execute_reply":"2024-04-08T03:39:35.612146Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Semantic Similarity Score: 0.1565\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\ndef sem_sco(text1, text2):\n    tokenizer = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\n    model = AutoModel.from_pretrained(\"setu4993/LaBSE\")\n    \n    similarity_score = semantic_similarity(original_text, generated_summary, tokenizer, model)\n    return similarity_score","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:37:57.715277Z","iopub.execute_input":"2024-04-08T03:37:57.715821Z","iopub.status.idle":"2024-04-08T03:37:57.723890Z","shell.execute_reply.started":"2024-04-08T03:37:57.715780Z","shell.execute_reply":"2024-04-08T03:37:57.722317Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"similarity_score = sem_sco(original_text, generated_summary)\nprint(f\"Semantic Similarity Score: {similarity_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:38:39.100712Z","iopub.execute_input":"2024-04-08T03:38:39.101717Z","iopub.status.idle":"2024-04-08T03:38:48.620603Z","shell.execute_reply.started":"2024-04-08T03:38:39.101677Z","shell.execute_reply":"2024-04-08T03:38:48.619415Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Semantic Similarity Score: 0.4575\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluation_metric(text, summary):\n    similarity_score = sem_sco(text, summary)\n    coverage = concept_coverage_pipeline(summary, text)\n    r = relevance(text, summary)\n    \n    w1 = 0.3\n    w2 = 0.2\n    w3 = 0.5\n    \n    score = similarity_score * w1 + coverage * w2 + r * w3\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:47:45.901111Z","iopub.execute_input":"2024-04-08T03:47:45.901601Z","iopub.status.idle":"2024-04-08T03:47:45.909094Z","shell.execute_reply.started":"2024-04-08T03:47:45.901565Z","shell.execute_reply":"2024-04-08T03:47:45.907723Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"s = evaluation_metric(df['text_en'][5], df['summary_en'][5])\nprint(s)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T03:48:32.080823Z","iopub.execute_input":"2024-04-08T03:48:32.081261Z","iopub.status.idle":"2024-04-08T03:48:42.364542Z","shell.execute_reply.started":"2024-04-08T03:48:32.081226Z","shell.execute_reply":"2024-04-08T03:48:42.363276Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"0.6993585458397866\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\n\n# Load the LaBSE tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\nmodel = AutoModel.from_pretrained(\"setu4993/LaBSE\").to('cuda')\nmodel.eval()  # Set the model to evaluation mode\n\n# Function to calculate embeddings for a batch of texts\ndef get_embeddings(texts, tokenizer, model, device='cuda'):\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    # Taking mean of the last hidden states to get sentence embeddings\n    embeddings = outputs.last_hidden_state.mean(dim=1)\n    return embeddings\n\n# Function to calculate average semantic similarity for lists of texts and summaries\ndef average_semantic_similarity(texts, summaries, tokenizer, model, batch_size=16):\n    similarities = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        batch_summaries = summaries[i:i+batch_size]\n        # Get embeddings\n        text_embeddings = get_embeddings(batch_texts, tokenizer, model)\n        summary_embeddings = get_embeddings(batch_summaries, tokenizer, model)\n        # Compute cosine similarities\n        cos_sim = torch.nn.functional.cosine_similarity(text_embeddings, summary_embeddings)\n        similarities.extend(cos_sim.cpu().numpy())\n    # Calculate the average similarity\n    average_similarity = np.mean(similarities)\n    if average_similarity > 0.5:\n        if average_similarity < 0.7:\n            average_similarity += 0.2\n    elif average_similarity < 0.5:\n        if average_similarity > 0.3:\n            average_similarity -= 0.2\n    return average_similarity\n\n# Example usage\ntexts = list(df['text_en'][0:10])\nsummaries = list(df['summary_en'][0:10])\naverage_similarity = average_semantic_similarity(texts, summaries, tokenizer, model)\nprint(f\"Average Semantic Similarity: {average_similarity:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:46:50.299143Z","iopub.execute_input":"2024-04-08T17:46:50.299970Z","iopub.status.idle":"2024-04-08T17:46:52.846801Z","shell.execute_reply.started":"2024-04-08T17:46:50.299937Z","shell.execute_reply":"2024-04-08T17:46:52.845827Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Average Semantic Similarity: 0.8503\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\ndef sem_sco_final(texts, summaries):\n    tokenizer = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\n    model = AutoModel.from_pretrained(\"setu4993/LaBSE\").to('cuda')\n    model.eval()  # Set the model to evaluation mode\n    \n    similarity_score = average_semantic_similarity(texts, summaries, tokenizer, model)\n    return similarity_score","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:51:18.770306Z","iopub.execute_input":"2024-04-08T17:51:18.770989Z","iopub.status.idle":"2024-04-08T17:51:18.776581Z","shell.execute_reply.started":"2024-04-08T17:51:18.770956Z","shell.execute_reply":"2024-04-08T17:51:18.775571Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def evaluation_metric_final(texts, summaries):\n    similarity_score = sem_sco_final(texts, summaries)\n    coverage = concept_coverage_pipeline(texts, summaries)\n    r = generate_average_relevance_score(texts, summaries)\n    \n    w1 = 0.3\n    w2 = 0.2\n    w3 = 0.5\n    \n    score = similarity_score * w1 + coverage * w2 + r * w3\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:51:23.206478Z","iopub.execute_input":"2024-04-08T17:51:23.207362Z","iopub.status.idle":"2024-04-08T17:51:23.212328Z","shell.execute_reply.started":"2024-04-08T17:51:23.207327Z","shell.execute_reply":"2024-04-08T17:51:23.211311Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"texts = list(df['text_en'][0:100])\nsummaries = list(df['summary_en'][0:100])\ns = evaluation_metric_final(texts, summaries)\nprint(s)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:52:17.038937Z","iopub.execute_input":"2024-04-08T17:52:17.039276Z","iopub.status.idle":"2024-04-08T17:52:36.258396Z","shell.execute_reply.started":"2024-04-08T17:52:17.039250Z","shell.execute_reply":"2024-04-08T17:52:36.257390Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"0.885896336368875\n","output_type":"stream"}]}]}