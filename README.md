# Multilingual & Cross-Lingual Text Summarization

## Project Overview
This project focuses on multilingual and cross-lingual text summarization, specifically between English and Marathi, using advanced transformer-based models. It includes datasets, model fine-tuning, evaluation metrics, and case studies.

## Folder Structure
- **Dataset**: Contains English and Marathi Monolingual Summarization Dataset and Cross Summarization Dataset from the Cross Sum Dataset.
- **English & Marathi Summarization**: Code for fine-tuning the 6 selected models.
- **Evaluation Metric**: Code for creating the Relevance Dataset, model training, evaluation, and the complete code for Evaluation Metric with Concept Coverage and Semantic Similarity, including testing.
- **Hindi Case Study**: Fine-tuning and evaluation of the mBART-50 model on the Hindi language dataset.
- **LLM Evaluation**: Code for evaluating GPT, mT0, and Gemini.
- **Evaluations Code**: Code used to obtain Metric scores for monolingual summarization models and cross-lingual approaches.

## Key Features
- **Model Fine-Tuning**: Utilizes Pegasus, T5, BART for English; IndicBART, mT5, mBART for Marathi.
- **Cross-Lingual Summarization**: Incorporates M2M-100 for translation, enhancing cross-lingual summarization capabilities.
- **Novel Evaluation Metric**: Combines concept coverage, semantic similarity, and relevance for assessing summarization quality.
- **Large Language Model Comparisons**: Includes comparative analysis with models such as GPT-4, GPT-3.5, mT0-base, and Gemini Pro.
- **Hindi Case Study**: Extends methodologies to Hindi, demonstrating versatility.

## Usage
1. Clone the repository: `git clone https://github.com/Shlok-D/BTech-Project.git`

## Observations
All observations are based on multiple reruns and extensive experimentation, ensuring robust results.
